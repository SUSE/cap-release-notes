// Start attribute entry list (Do not edit here! Edit in entities.adoc)
ifdef::env-github[]
:suse: SUSE
:product: {suse} Cloud Applications Platform
:version: 1.1
:rn-url: https://www.suse.com/releasenotes
:doc-url: https://www.suse.com/documentation/cloud-application-platform-1
:deployment-url: https://www.suse.com/documentation/cloud-application-platform-1/book_cap_deployment/data/book_cap_deployment.html
:caasp: {suse} Containers as a Service Platform
:caaspa: {suse} CaaS Platform
:ostack: OpenStack
:cf: Cloud Foundry
:scc: {suse} Customer Center
:azure: Microsoft Azure
endif::[]
// End attribute entry list


[id='sec.azure']
== Installing {product} on {azure}

{product} {version} supports running on
https://azure.microsoft.com/en-us/services/container-service[Microsoft Azure
Container Service (ACS)]. This document describes how to prepare the ACS
environment for deployment of {product}. For more detailed product
information and instructions, see the {product} documentation at {doc-url}.


[id='sec.azure.requirement']
=== Requirements

To setup the {azure} environment, use the command line client `az`.
On {suse} systems, it can be installed by executing the following commands:

[source,bash]
----
$ sudo zypper install -y curl
$ sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc
$ sudo sh -c 'echo -e "[azure-cli]\nname=Azure CLI\nbaseurl=https://packages.microsoft.com/yumrepos/azure-cli\nenabled=1\ntype=rpm-md\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc" > /etc/zypp/repos.d/azure-cli.repo'
$ sudo zypper refresh
$ sudo zypper install -y azure-cli
----

Then log in to {azure}:

[source,bash]
----
$ az login
----

[id='sec.azure.kubernetes']
=== Setting Up Kubernetes

. Extract the subscriptions ID and write it down:
+
[source,bash]
----
$ az account show --query "{ subscription_id: id }"
----

. Create a service principal and write down the _password_ and _appId_. The
term _<subscription-id>_ needs to be replaced by the _subscription_id_ noted
in the in the previous step. If the {azure} service is supposed to be located
in a data center other than a Western USA data center `westus` needs to be
replaced.
+
[source,bash]
----
$ export SUBSCRIPTION_ID=<subscription-id>
$ az account set --subscription $SUBSCRIPTION_ID
$ az group create --name scf-resource-group --location westus
$ az ad sp create-for-rbac --role Contributor --scopes "/subscriptions/$SUBSCRIPTION_ID/resourceGroups/scf-resource-group"
----

. Create a container service in {azure} and replace the terms
_<password>_ and _<application-id>_ with the one noted from the previous
step. Additionally, the phrase _<your-public-ssh-key>_ should be replaced by
the public SSH key to be used:
+
[source,bash]
----
$ az acs create \
       --name scf-container-service \
       --resource-group scf-resource-group \
       --orchestrator-type Kubernetes \
       --dns-prefix "myscf" \
       --master-count 1 \
       --admin-username scf-admin \
       --agent-count 3 \
       --client-secret <password> \
       --ssh-key-value "<your-public-ssh-key>" \
       --service-principal <application-id> \
       --master-vm-size Standard_D2_v2
----

. Prepare kubernetes environment. If there is already another running
Kubernetes it is advised to backup the old configuration located in
`~/.kube/`.
+
[source,bash]
----
$ az acs kubernetes get-credentials --resource-group="scf-resource-group" --name="scf-container-service"
----
+
The above command can fail if the SSH key is password protected. Adding
the _<private_key_file_name>_ to the ssh-agent will fix the issue:
+
[source,bash]
----
$ ssh-add ~/.ssh/<private_key_file_name>
----


. The Kubernetes config can be verified by listing all the current pods. The minimal required Kubernetes version is 1.6.
+
[source,bash]
----
$ kubectl get pods --all-namespaces
----

* Enable cgroup swap accounting by running the following commands. This will
reboot some of the machines which takes some time. This also does require
the commands `sed` and `jq`.
+
[source,bash]
----
$ sudo zypper in -y jq sed
$ az vm list -g "scf-resource-group" | jq '.[] | select (.tags.poolName | contains("agent")) | .name' | \
  xargs -i{} az vm run-command invoke \
    --resource-group "scf-resource-group" \
    --command-id RunShellScript \
    --scripts "sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=\"console=tty1 console=ttyS0 earlyprintk=ttyS0 rootdelay=300\"/GRUB_CMDLINE_LINUX_DEFAULT=\"console=tty1 console=ttyS0 earlyprintk=ttyS0 rootdelay=300 swapaccount=1\"/g' /etc/default/grub.d/50-cloudimg-settings.cfg" --name {}

$ az vm list -g "scf-resource-group" | jq '.[] | select (.tags.poolName | contains("agent")) | .name' | \
  xargs -i{} az vm run-command invoke \
    --resource-group "scf-resource-group" \
    --command-id RunShellScript \
    --scripts "sudo update-grub" --name {}

$ az vm list -g "scf-resource-group" | jq '.[] | select (.tags.poolName | contains("agent")) | .name' | \
  xargs -i{} az vm restart --no-wait \
    --resource-group "scf-resource-group" \
    --name {}
----

. Create a new public IP address and write it down. It is needed for the
later deployment.
+
[source,bash]
----
$ az network public-ip create --resource-group scf-resource-group --name scf-public-ip --allocation-method Static
----


. Extract the name of one of the kubes agent NIC and write it down
+
[source,bash]
----
$ az network nic list --resource-group scf-resource-group | grep name | grep agent | grep 0
----

. Attach the public IP to the master kubes node by replacing the term
_<nic-name>_ by the one noted down before and write down the value of
_privateIpAddress_. This private IP address is needed for the later
deployment.
+
[source,bash]
----
$ az network nic ip-config update --resource-group scf-resource-group --nic-name <nic-name> --name ipconfig1 --public-ip-address scf-public-ip
----

. Extract the security group name of the master network security group and
write it down:
+
[source,bash]
----
$ az network nsg list --resource-group=scf-resource-group | jq -r '.[] | .name' | grep master
----

. Create required security groups and replace _<security-group>_ with the name
you wrote down in the previous step:
+
[source,bash]
----
$ export NSG_NAME=<security-group>
$ az network nsg rule create --resource-group scf-resource-group --priority 200 --nsg-name $NSG_NAME --name scf-80 --direction Inbound --destination-port-ranges 80 --access Allow
$ az network nsg rule create --resource-group scf-resource-group --priority 201 --nsg-name $NSG_NAME --name scf-443 --direction Inbound --destination-port-ranges 443 --access Allow
$ az network nsg rule create --resource-group scf-resource-group --priority 202 --nsg-name $NSG_NAME --name scf-4443 --direction Inbound --destination-port-ranges 4443 --access Allow
$ az network nsg rule create --resource-group scf-resource-group --priority 203 --nsg-name $NSG_NAME --name scf-2222 --direction Inbound --destination-port-ranges 2222 --access Allow
$ az network nsg rule create --resource-group scf-resource-group --priority 204 --nsg-name $NSG_NAME --name scf-2793 --direction Inbound --destination-port-ranges 2793 --access Allow
----

[id='sec.azure.deploy']
=== Deploying {product}

For the next steps the public and private IP addresses written down before
are required.

When the Kubernetes environment is prepared, proceed with deployment of
{product} as described in the _Deployment Guide_ at {deployment-url}.

*NOTE:* When deploying to {azure}, you will need to set the Garden rootfs
driver to `overlay-xfs` (the default would be `btrfs`). You can do that by
setting the following key in the `scf-config-values.yaml`:
+
[source,yaml]
----
env:
  GARDEN_ROOTFS_DRIVER: "overlay-xfs"
----
