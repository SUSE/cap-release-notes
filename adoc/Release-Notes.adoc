// Start attribute entry list (Do not edit here! Edit in entities.adoc)
ifdef::env-github[]
:suse: SUSE
:current-year: 2019
:product: {suse} Cloud Application Platform
:version: 1.5.1
:rn-url: https://www.suse.com/releasenotes
:doc-url: https://documentation.suse.com/suse-cap/1
:deployment-url: https://documentation.suse.com/suse-cap/1/html/cap-guides/part-cap-deployment.html
:caasp: {suse} Containers as a Service Platform
:caaspa: {suse} CaaS Platform
:ostack: OpenStack
:cf: Cloud Foundry
:scf: {suse} {cf}
:k8s: Kubernetes
:scc: {suse} Customer Center
:azure: Microsoft Azure
:aks: Azure {k8s} Service
:aksa: AKS
:aws: Amazon Web Services
:awsa: AWS
:eks: Amazon Elastic Container Service for Kubernetes
:eksa: Amazon EKS
:mysql: MySQL
:mariadb: MariaDB
:postgre: PostgreSQL
:redis: Redis
:mongo: MongoDB
:ng: NGINX
endif::[]
// End attribute entry list

[id='sec.major-change']
== Major Changes

[id='sec.1_5_1']
=== Release 1.5.1, December 2019

[id='sec.1_5_1.new']
==== What Is New?
* {scf} has been updated to version 2.19.1:
** Support for Eirini SSH feature
** Support for external Cloud Controller and UAA database configuration
** Ingress controller now available for UAA embedded in SCF
** AUDIT_WRITE capabilities added for CRI-O
* The Stratos UI has been updated to version 2.6.0:
** For a full list of features and fixes see https://github.com/SUSE/stratos/releases/tag/2.6.0
* Stratos Metrics has been updated to version 1.1.1:
** For the full list of updates see https://github.com/SUSE/stratos-metrics/blob/master/CHANGELOG.md#111

[id='sec.1_5_1.feature']
==== Features and Fixes

* Fixed Eirini on GKE, EKS and Kubernetes clusters using CRI-O
* Eirini will use SLE15 as its default stack
* eirini-cert-copier no longer appears when scheduler is diego
* Enforced odd number of mysql replicas for HA scenarios to improve consistency with PXC
* Improved mysql-proxy active/passive handling
* Fixed apiVersion in Chart yaml(s) to point to Helm API version (v1)
* Turned binlog on for pxc config to enable transaction recovery
* Moved to stack-associated (or stackful) buildpacks, away from multi-stack
* Enabled BPM for bits-service for reliability
* garden.disable_swap_limit set to "true" to remove the need for swap accounting
* Includes these {cf} component versions:
** app-autoscaler: 1.2.1
** bits-services: 2.28.0
** bpm: 1.1.0
** capi: 1.83.0
** cf-acceptance-tests: 9.5
** cf-deployment: 9.5
** cf-mysql: 36.15.0
** cf-routing: 0.188.0
** cf-sle12: 1.81.61
** cf-sle15: 10.70
** cf-smoke-tests: 40.0.112
** cf-syslog-drain: 10.2
** cf-usb: 1.0.1
** cflinuxfs3: 0.141.0
** credhub: 2.4.0
** diego: 2.34.0
** eirini: 0.0.23
** garden-runc: 1.19.3
** groot-btrfs: 1.0.5
** log-cache: 2.2.2
** loggregator: 105.5
** loggregator-agent: 3.9
** mapfs: 1.1.0
** nats: 27
** nfs-volume: 1.5.2
** postgres-release: 26
** pxc: 0.18.0
** scf-helper: 1.0.7
** statsd-injector: 1.10.0
** uaa: 72.0
* Buildpacks:
** binary-buildpack: 1.0.35
** dotnetcore-buildpack: 2.3.0
** go-buildpack: 1.9.2
** java-buildpack: 4.24.0
** nginx-buildpack: 1.1.0
** nodejs-buildpack: 1.7.1
** php-buildpack: 4.4.0
** python-buildpack: 1.6.37
** ruby-buildpack: 1.8.1
** staticfile-buildpack: 1.5.0

[id='sec.1_5_1.issue']
==== Known Issues

[IMPORTANT] 
====
* For upgrades from {product} 1.5 to 1.5.1, if the `mysql` roles of `uaa` and `scf` are in high availability mode, it is recommended to first scale them to single availability. Performing an upgrade with these roles in HA mode runs the risk of encountering potential database migration failures, which will lead to pods not starting. This does not impact usage when connected to external databases.

** For general information about upgrading to {product} 1.5.1, see https://documentation.suse.com/suse-cap/1.5.1/single-html/cap-guides/#sec-cap-update

** For clusters currently using `config.HA`, see https://documentation.suse.com/suse-cap/1.5.1/single-html/cap-guides/#sec-cap-upgrade-config-ha

** For clusters currently using custom sizing, see https://documentation.suse.com/suse-cap/1.5.1/single-html/cap-guides/#sec-cap-upgrade-custom-sizing

** For clusters in single availability, see https://documentation.suse.com/suse-cap/1.5.1/single-html/cap-guides/#sec-cap-upgrade-no-ha
====

* In circumstances where the `uaa` pod may fail to start due to database migration failures, manual intervention is required to track the last completed transaction in the `uaadb` database, update the `schema_version` table with the record of the last completed transaction, then restart the migration. Please contact support for further instructions.

[id='sec.1_5']
=== Release 1.5, September 2019

[id='sec.1_5.new']
==== What Is New?

* {scf} has been updated to version 2.18.0:
** PXC (Percona XtraDB Cluster) replaces `cf-mysql` for database management -- please read the Known Issues section for this version on deployment and upgrade changes
** Ability to set `config.HA_strict=false` in combination with `config.HA=true` to allow lowering the sizing count for a role below what is required for HA
** UAA can now be deployed as embedded in the `cf` namespace, allowing for a single step deployment -- please read the Known Issues section for this version on deployment limitations
* The Stratos UI has been updated to version 2.5.1:
** Tech preview for helm feature
** Add custom welcome message on endpoints page
** Add support for connecting {caasp} V4 endpoints
** Refinements to the Autoscaler UI
** For a full list of features and fixes see https://github.com/SUSE/stratos/releases/tag/2.5.1.

For information about deploying and administering {product}, see the product manuals at
{doc-url}.

[id='sec.1_5.feature']
==== Features and Fixes
* `cf-deployment` has been updated to version 9.5.0.
* Eirini updated to 0.0.14
* Removed the `cluster-admin` role binding for the eirini service account
* Removed deprecated `cflinuxfs2` -- please read the Known Issues section for this version as to why
* Switched over to `PXC` from `cf-mysql` for database management
* Includes these {cf} component versions:
** app-autoscaler: 1.2.1
** bits-service: 2.28.0
** bpm: 1.1.0
** capi: 1.83.0
** cf-deployment: 9.5
** cf-mysql: 36.15.0
** cf-routing: 0.188.0
** cf-sle12: 1.81.26
** cf-sle15: 10.28
** cf-smoke-tests: 40.0.112
** cf-syslog-drain: 10.2
** cf-usb: 1.0.1
** cflinuxfs3: 0.118.0
** credhub: 2.4.0
** diego: 2.34.0
** eirini: 0.0.14
** garden-runc: 1.19.3
** groot-btrfs: 1.0.5
** log-cache: 2.2.2
** loggregator: 105.5
** loggregator-agent: 3.9
** mapfs: 1.1.0
** nats: 27
** nfs-volume: 1.5.2
** postgres-release: 26
** pxc: 0.18.0
** scf-helper: 1.0.3
** statsd-injector: 1.10.0
** uaa: 72.0
* Buildpacks:
** binary-buildpack: 1.0.33
** dotnet-core-buildpack: 2.2.13
** go-buildpack: 1.8.42
** java-buildpack: 4.20.0 
** nginx-buildpack: 1.0.15
** nodejs-buildpack: 1.6.53
** php-buildpack: 4.3.80
** python-buildpack: 1.6.36
** ruby-buildpack: 1.7.42
** staticfile-buildpack: 1.4.43

[id='sec.1_5.issue']
==== Known Issues

[IMPORTANT]
====
In order to deploy {product} 1.5 or upgrade from {product} 1.4.1 in an HA configuration, you will first need to start the `mysql` role with 1 instance to be able to migrate from `cf-mysql` to `PXC`. This is based on upstream instructions but based on what we've seen with other components that rely on the database, such as Credhub, scaling all database roles into single availability helps with a stable migration and deployment.

Steps for a fresh install of {product} 1.5 in the default HA configuration: 

[arabic]
.. Install HA UAA but start the `mysql` role with a count of 1 as a transition step. (In the commands below, `susecf-uaa` and `susecf-scf` are assumed to be the release names and `suse/uaa` and `suse/cf` are the chart names in the repository. Adjust the release names accordingly to suit your configuration.). By specifying `config.HA=true` the instance count of all roles will be set to the minimum required for HA mode, otherwise referred to as the default HA configuration. Additionally, specify `config.HA_strict=false` along with `sizing.mysql.count=1` so that there is only a single `mysql` role.
+
[source,bash]
----
helm install --name susecf-uaa --namespace uaa suse/uaa -f <values.yaml> --set config.HA=true \
--set config.HA_strict=false --set sizing.mysql.count=1 --version 2.18.0
----
+
.. Set the value of the `secrets.UAA_CA_CERT` to pass your `uaa` secret and certificate to `scf`.
+
[source,bash]
----
SECRET=$(kubectl get pods --namespace uaa \
--output jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')
CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
--output jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
----
+
.. Similarly, install HA SCF but start the `mysql` role with a count of 1 as a transition step. By specifying `config.HA=true` the instance count of all roles will be set to the minimum required for HA mode, otherwise referred to as the default HA configuration. Additionally, specify `config.HA_strict=false` along with `sizing.mysql.count=1` so that there is only a single `mysql` role.
+
[source,bash]
----
helm install --name susecf-scf --namespace scf suse/cf -f <values.yaml> --set config.HA=true \ 
--set config.HA_strict=false --set sizing.mysql.count=1 --set "secrets.UAA_CA_CERT=${CA_CERT}" \
--version 2.18.0
----
+
.. Scale the `mysql` role up to the default HA configuration.
+
[source,bash]
----
helm upgrade susecf-uaa --namespace uaa suse/uaa -f <values.yaml> --set config.HA_strict=true \ 
--set config.HA=true --version 2.18.0
helm upgrade susecf-scf --namespace scf suse/cf -f <values.yaml> --set config.HA_strict=true \ 
--set config.HA=true --set "secrets.UAA_CA_CERT=${CA_CERT}" --version 2.18.0
----

Steps to upgrade from {product} 1.4.1 to 1.5 will depend on the configuration of your current deployment. If the `mysql` roles of your deployment are:

* In high availability mode by setting `config.HA` to `true` see https://documentation.suse.com/suse-cap/1.5/single-html/cap-guides/#sec-cap-upgrade-config-ha
* In high availability mode by configuring custom sizing values, see https://documentation.suse.com/suse-cap/1.5/single-html/cap-guides/#sec-cap-upgrade-custom-sizing
* In single availability mode, see https://documentation.suse.com/suse-cap/1.5/single-html/cap-guides/#sec-cap-upgrade-no-ha

====

IMPORTANT: If you are using a buildpack that uses the same name as a shipped buildpack, you will need to rename it to a unique name. Based on our existing model of stackless buildpacks, any buildpack name already in use is considered reserved. 

IMPORTANT: As of {scf} 2.18.0, since our `cf-deployment` version is 9.5, the `cflinuxfs2` stack is no longer supported, as was advised in {scf} 2.17.1 or {product} 1.4.1. The `cflinuxfs2` buildpack is no longer shipped, but if you are upgrading from an earlier version, `cflinuxfs2` will not be removed. However, for migration purposes, we encourage all admins to move to `cflinuxfs3` or `sle15` as newer buildpacks will not work with the deprecated `cflinuxfs2`. If you still want to use the older stack, you will need to build an older version of a buildpack to continue for the app to work, but you will be unsupported. (If you are running on `sle12`, we will be retiring that stack in a future version so start planning your migration to `sle15`.)

[IMPORTANT] 
====
As of {scf} 2.18.0, `cf push` with `eirini` does not work on {eks} and Google Kubernetes Engine (GKE) by default. To get `cf push` to work with {eks} and GKE, you need to apply a workaround of deleting a webhook by doing the following:

[source,bash]
----
kubectl delete mutatingwebhookconfigurations eirini-x-mutating-hook-eirini
----

Deleting the webhook means that the `eirini-persi` service would not be available. Note that this workaround is not needed on {aks}.
====

* When deploying {scf} with Eirini, the `cflinuxfs3` stack is the only one that works as part of this tech preview.

* If you are using the `uaa` embedded in the `suse/cf` chart, note that automatic ingress creation via helm will not work at present. Therefore, the ingress controller will not work with embedded `uaa` but but the chart can be deployed with {k8s} `LoadBalancer` services.

* On occasion, the `credhub` pod may fail to start due to database migration failures; this has been spotted intermittently on {aks} and to a lesser extent, other public clouds. In these situations, manual intervention is required to track the last completed transaction in `credhub_user` database and update the flyway schema history table with the record of the last completed transaction. Please contact support for further instructions.

* In some situations, the `autoscaler-metrics` pod may fail to reach a fully ready state due to a Liquibase error: `liquibase.exception.LockException: Could not acquire change log lock`. When this occurs, refer to Part V of the {product} Deployment Guide to troubleshoot and resolve this issue at {doc-url}.

[id='sec.1_4_1']
=== Release 1.4.1, July 2019

[id='sec.1_4_1.new']
==== What Is New?

* {scf} has been updated to version 2.17.1.

[id='sec.1_4_1.feature']
==== Features and Fixes

* Set the default value of `AZ_LABEL_NAME` to `failure-domain.beta.kubernetes.io/zone`.
* Simplified service accounts and pod security policies.
* Switched to log-cache for container metrics.
* Implemented a patch to squash Cloud Controller database migrations.
* Fixed version and SHA1 of `cf-mysql-release` tied to version 36.15.0.
* Fixed TLS issues in `log-cache`.

* Includes these {cf} component versions:

** app-autoscaler: 1.2.1
** bits-service: 2.26.0
** bpm: 1.0.0
** capi: 1.79.0
** cats: 7.11
** cf-deployment: 7.11
** cf-mysql: 36.15.0
** cf-routing: 0.187.0
** cf-sle12: 1.75.11
** cf-smoke-tests: 40.0.51
** cf-syslog-drain: 10.0
** cf-usb: 1.0.1
** cflinuxfs2: 1.281.0
** cflinuxfs3: 0.108.0
** credhub: 2.1.2
** diego: 2.30.0
** eirini: 0.0.4
** garden-runc: 1.19.1
** groot-btrfs: 1.0.4
** kubectl: 1.9.6
** loggregator: 105.2
** loggregator-agent: 3.9
** nats: 26
** nfs-volume: 1.5.2
** postgres-release: 26
** scf-helper: 1.0.2
** statsd-injector: 1.9.0
** uaa: 68.0
* Buildpacks:
** binary-buildpack: 1.0.32
** dotnet-core-buildpack: 2.2.12
** go-buildpack: 1.8.41
** java-buildpack: 4.19.1 
** nginx-buildpack: 1.0.14
** nodejs-buildpack: 1.6.51
** php-buildpack: 4.3.77
** python-buildpack: 1.6.34
** ruby-buildpack: 1.7.40
** staticfile-buildpack: 1.4.43

[id='sec.1_4_1.issue']
==== Known Issues

* `cf-deployment` 7.11 is the last {cf} version that supports the `cflinuxfs2`
  stack. The `cflinuxfs2` and `sle12` stacks are deprecated in favor of
  `cflinuxfs3` and `sle15` respectively. Start planning to migrate applications
  to the newer stacks for futureproofing, as the older stacks will be removed
  in a future release. The Stack Auditor plugin for `cf` can help with this
  migration (see https://docs.cloudfoundry.org/adminguide/stack-auditor.html).


[id='sec.1_4']
=== Release 1.4, May 2019

[id='sec.1_4.new']
==== What Is New?

* {scf} has been updated to version 2.16.4:
** A tech preview of Eirini is available. To enable Eirini, follow the instructions from https://github.com/SUSE/scf/wiki/Eirini. 
** Added SLE15 stack.
** Added feature flags to enable roles such as autoscaler, cf-usb, credhub and eirini.
** Added Sync Integration Test Suite (SITS).
** Added support for NGINX Ingress Controller with customizable Ingress via user supplied annotations.
** Added .net-core buildpack (2.2.7).
* The Stratos UI has been updated to version 2.4:
** For a full list of features and fixes see https://github.com/SUSE/stratos/releases/tag/2.4.0.

For information about deploying and administering {product}, see the product manuals at
{doc-url}.


[id='sec.1_4.feature']
==== Features and Fixes

* cf-mysql-release has been pinned at version 36.15.0 to avoid intermittent database connectivity errors in HA setup.

* Changed app autoscaler-postgres to a non-HA setup due to a known limitation - see https://github.com/cloudfoundry/postgres-release/#known-limitations.

* The app autoscaler services are no longer deployed as {k8s} services of type LoadBalancer and therefore, are not exposed on public IP addresses or hostnames.

* Fixed autoscaler to perform SSL validation.

* Fixed autoscaler to listen to cluster internal CF API endpoint.

* The default `nproc` limits for the vcap user for all SCF roles have been bumped to 1024/2048 (soft/hard). You can use different limits by setting `kube.limits.nproc.soft` and `kube.limits.nproc.hard` in the Helm chart values. 

* Cleaned up role readiness probe outputs.

* Fixed the test for an insecure Docker registry (uses tcpdomain for the route).

* Includes these {cf} component versions:
** app-autoscaler: 1.0.0
** bits-service: 2.26.0
** bpm: 1.0.0
** capi: 1.79.0
** cf-deployment: 6.10
** cf-mysql: 36.15.0
** cf-routing: 0.184.0
** cf-sle12: 1.75.11
** cf-smoke-tests: 40.0.44
** cf-syslog-drain: 8.1
** cf-usb: 1.0.1
** cflinuxfs2: 1.281.0
** cflinuxfs3: 0.81.0
** credhub: 2.1.2
** diego: 2.25.0
** eirini: 0.0.4
** garden-runc: 1.17.2
** groot-btrfs: 1.0.4
** kubectl: 1.9.6
** loggregator: 104.4
** loggregator-agent: 3.2
** nats: 26
** nfs-volume: 1.5.2
** postgres-release: 26
** scf-helper: 1.0.2
** cf-acceptance-tests: 
** statsd-injector: 1.5.0
** uaa: 68.0
* Buildpacks:
** binary-buildpack: 1.0.32
** dotnet-core-buildpack: 2.2.10
** go-buildpack: 1.8.36
** java-buildpack: 4.19.1 
** nginx-buildpack: 1.0.11
** nodejs-buildpack: 1.6.49
** php-buildpack: 4.3.75
** python-buildpack: 1.6.32
** ruby-buildpack: 1.7.38
** staticfile-buildpack: 1.4.42

[id='sec.1_4.issue']
==== Known Issues

* The instructions for enabling Eirini can be found at https://github.com/SUSE/scf/wiki/Eirini.

* Currently, Eirini does not work on {k8s} environments running cri-o. To make Eirini work, use the Docker runtime.

* Resuming a past practice, with {product} 1.4, use the complete command: `helm upgrade --force --recreate-pods` for an upgrade. This will reintroduce downtime for apps but without `--recreate-pods`, multiple versions of statefulsets may co-exist which can cause incompatibilities between dependent statefulsets, and result in a broken upgrade. This applies to Stratos pods as well.

* With the introduction of feature flags, setting `sizing.<role>.count` to enable/disable a feature is no longer supported. You must explicitly set `enable.<feature>` to `true` or `false` to enable/disable a feature. As an example, if you had enabled credhub or autoscaler in {product} 1.3.1, then you must add `enable.credhub=true` or `enable.autoscaler=true` during the `helm upgrade`. If you had previously set `sizing.<role>.count` to `1` you can remove that as the new minimum setting is `1`. Conversely, if you had disabled a feature in {product} 1.3.1, you should remove the corresponding sizing setting and, instead, explicitly set `enable.<feature>=false` during the upgrade. If you would like to deploy more than `1` instance of an optional role, you would need to use an appropriate value for `sizing.<role>.count` in addition to using the feature flag.

* If autoscaler was enabled in {product} 1.3.1, you must specify `sizing.autoscaler_postgres.disk_sizes.postgres_data=100` during the helm upgrade to avoid upgrade errors. Alternatively, you can disable the autoscaler before the upgrade and re-enable after the upgrade is finished. Without any of these workarounds, the upgrade would fail with `Error: UPGRADE FAILED: StatefulSet.apps "autoscaler-postgres" is invalid` message.

* If you are using the NGINX Ingress Controller and seeing `Request Entity Too Large` errors, you should bump up the ingress proxy body size to an appropriate value by setting the `ingress.annotations` key in helm chart values as in the following:
+
[source]
----
  ingress:
     annotations:
       nginx.ingress.kubernetes.io/proxy-body-size: 64m
----

* If during an upgrade the `post-deployment` job does not complete, re-apply the `helm upgrade`.

* On GKE, the swap accounting related kernel boot parameter changes on the worker nodes may not be retained as GCP may automatically re-provision nodes to perform upgrades or repairs. One option you may want to consider is to set up the GKE cluster with `auto-repair` and `auto-upgrade` set to `false` to reduce the ephemeral nature of the GKE nodes. See https://cloud.google.com/kubernetes-engine/docs/concepts/node-images#modifications for more details.

* On GKE you should set up the {k8s} storage class to be backed by an SSD instead of a standard disk. 


[id='sec.1_3_1']
=== Release 1.3.1, February 2019

[id='sec.1_3_1.new']
==== What Is New?

* {scf} has been updated to version 2.15.2:
** Default PodSecurityPolicies (PSPs) come with the helm charts
** cflinuxfs3 now available as a stack
** Added nginx buildpack
** Support added for placement zones & isolation segments
* The Stratos UI has been updated to version 2.3:
** Support for extensions
** For a full list of features and fixes see https://github.com/SUSE/stratos/releases/tag/2.3.0.

For information about deploying and administering {product}, see the product manuals at
{doc-url}.


[id='sec.1_3_1.feature']
==== Features and Fixes

* App-AutoScaler no longer depends on hairpin
* CredHub on {azure} is now supported
* Corrected service name to work with `syslog` drains
* Certificates rely on correct FQDN for UAA
* Removed obsolete key and diego-cell readiness probe from `role-manifest.yml`
* Changed one variable name to align with upstream practices--this may require changes to sizing:
** `cf-routing` replaces `routing`
* Includes these {cf} component versions:
** app-autoscaler: 1.0.0
** bpm: 1.0.0
** capi: 1.66.0
** cf-deployment: 3.6.0
** cf-mysql: 36.15.0
** cf-routing: 0.180.0
** cf-sle12: 1.52.6
** cf-smoke-tests: 40.0.6
** cf-syslog-drain: 7.0
** cf-usb: 1.0.1
** cflinuxfs2: 1.266.0
** cflinuxfs3: 0.60.0
** credhub: 2.0.2
** diego: 2.16.0
** garden-runc: 1.16.3
** groot-btrfs: 1.0.4
** kubectl: 1.9.6
** loggregator: 103.1
** loggregator-agent: 2.0
** nats: 25
** nfs-volume: 1.2.0
** opensuse42: 1.8.6
** postgres-release: 26
** scf-helper: 1.0.1
** cf-acceptance-tests: 2.8
** statsd-injector: 1.3.0
** uaa: 60.2
** uaa-fissile: c9edf895
* Buildpacks:
** binary-buildpack: 1.0.30
** dotnet-core-buildpack: 2.0.3
** go-buildpack: 1.8.33
** java-buildpack: 4.17.2
** nginx-buildpack: 1.0.8
** nodejs-buildpack: 1.6.43
** php-buildpack: 4.3.70
** python-buildpack: 1.6.27
** ruby-buildpack: 1.7.31
** staticfile-buildpack: 1.4.39


[id='sec.1_3_1.issue']
==== Known Issues

* For {product} 1.3.1, during the helm upgrade from 1.3.0, the `--recreate-pods` is not required as the recent change to the active/passive model allowed for previously Unready pods to be upgraded. This will allow for zero app downtime from the previous version.

* For deployments on {eksa}: the {awsa} Service Broker (https://aws.amazon.com/partners/servicebroker/) should now be used instead of the deprecated `cf-brokers` wrapper.

* For custom PSPs, `SYS_RESOURCE` no longer needs to be specified under added capabilities in the `scf-config-values.yml`

* During an upgrade from 2.14 to 2.15.2, the GoRouter and the applications it routes to will be unavailable until the new GoRouter pods are ready. You can work around this by setting the following label on the existing GoRouter pod specs:
labels:
+
[source]
----
labels:
.. `app.kubernetes.io/component: "router"`
.. `skiff-role-name: "router"`
----

* The App-AutoScaler services are exposed as Kube services of type LoadBalancer but they should only be accessed via the GoRouter. Therefore, do not rely on the public IPs for these services on the load balancer or do not create separate DNS entries for them -- use the DNS entries associated with the GoRouter public service instead.

* Deletion of {mariadb} instances created with Minibroker can fail with timeouts. If an error appears, wait one minute and retry. If the `cf delete-service` command fails but the instance pods are removed from {k8s}, the service instance data can safely be removed with a `cf purge-service-instance` command.

* On {azure} it is recommended to run on instance types `Standard_DS4_v2` or larger due to the introduction of the cflinuxfs3 stack. It's also recommended to use Premium SSD for the storage class.

* If you notice application instances (long-running processes or "LRPs") improperly persisting and accepting traffic after update or scaling actions, there may be an instance of the cc-clock role that did not come up properly due to an incorrect internal protocol setting. To address this:

[arabic]
.. Create a file called `cc-clock-patch.yml` with the following contents:
+
[source,bash]
----
bosh:
   instance_groups:
   - name: cc-clock
     jobs:
     - name: cloud_controller_clock
       properties:
         cc:
           external_protocol: http 
----
.. Rerun the upgrade of the CAP deployment via a Helm command with this syntax: `helm upgrade scf suse/cf --reuse-values --namespace scf -f cc-clock-patch-yml --version 2.15.2`
.. For high-availability (HA) deployments, manually restart the cc-clock-N pods by deleting them one at a time to avoid app downtime; newer updated pods will be created automatically:
+
[source,bash]
----
kubectl delete pod - n scf cc-clock-0
kubectl delete pod - n scf cc-clock-1
kubectl delete pod - n scf cc-clock-2
----
.. For single availability deployments, since there's only one cc-clock pod, app downtime is unavoidable.

//-

* The URL of the internal `cf-usb` broker endpoint has been corrected from the duplicate name from the previous version. To reconnect with {scf}/{product}, brokers for {postgre} and {mysql} that use `cf-usb` will require the following manual fix after the upgrade:

[arabic]
.. Run `kubectl get secret --namespace scf` and copy the name of the secret (for example, `secrets-2.15.2-1`)
.. Run `cf service-brokers` to get the URL for the `cf-usb` host (for example, `https://cf-usb-cf-usb.scf.svc.cluster.local:24054`)
.. Get the current `CF_USB` password by running:
+
[source,bash]
----
kubectl get secret --namespace scf <SECRET_NAME> -o yaml | \
  grep \\scf-usb-password: | cut -d: -f2 | base64 -id
----
+
Replace `<SECRET_NAME>` with the name from the first step.
.. Finally, update the service broker:
+
[source,bash]
----
cf update-service-broker usb broker-admin <PASSWORD> \
  https://cf-usb.scf.svc.cluster.local:24054
----
+
Replace `<PASSWORD>` with the password from step 3. The URL is a modified
version of the URL from step 2: however, as the subdomain name, use
`cf-usb` instead of `cf-usb-cf-usb`.


[id='sec.1_3']
=== Release 1.3, November 2018

[id='sec.1_3.new']
==== What Is New?

* {scf} has been updated to version 2.14.5:
** Includes support for {awsa} Service Broker
** Centralized credential management with CredHub is now available to {cf} apps and compatible brokers (disabled by default)
** Automatically scaling resource with App-AutoScaler is now supported for {aks} and {eks} (disabled by default)
** Minibroker has gained support for {redis}, {mongo}, {mysql}, {postgre}, and {mariadb}
* The Stratos UI has been updated to version 2.2:
** There is a new metrics endpoint for keeping and exposing {cf} application and {k8s} metrics
** There are new views for {k8s} application, pod, and node metrics
** For a more detailed list of new features and fixes, see https://github.com/SUSE/stratos/releases/tag/2.2.0.

For information about deploying and administering {product}, see the product manuals at
{doc-url}.


[id='sec.1_3.feature']
==== Features and Fixes

* One {k8s} service per job. The service names will include both the instance group (previously the role) and job name, which impacts the role manifest YAML
* Changed two variable names to align with upstream practices--this may require changes to sizing:
** `diego-ssh` replaces `diego-access`
** `api-group` replaces `api`
* UAA charts now have affinity/antiaffinity logic
* Exposed SMTP_HOST & SMTP_FROM_ADDRESS variables to allow for account creation & password reset
* `consul` role removed due to redundancy
* {k8s} readiness check no longer looks for `hyperkube` explicitly
* Updated cluster role names to ensure no namespace conflicts in {k8s}
* Includes these {cf} component versions:
** UAA: v60.2
** cf-deployment: 2.7.0
** kubectl: 1.9.6
** capi-release: 1.61.0
** cflinuxfs2-release: v1.227.0
** cf-mysql-release: v36.15.0
** cf-opensuse42-release: 1.7.87
** cf-sle12-release: 1.51.115
** cf-smoke-tests-release: 40.0.5
** cf-syslog-drain-release: v7.0
** cf-usb: 7a45076
** diego-release: v2.12.1
** garden-runc-release: v1.15.1
** groot-btrfs: 305b068d
** loggregator-agent-release: v2.0
** loggregator-release: v103.0
** nats-release: v24
** nfs-volume-release: v1.2.0
** postgres-release: v26
** routing-release: 0.179.0
** scf-helper-release: b9fa59d
** cf-acceptance-tests: c83c97b9
** testbrain: 1.0.0-61-ga172cf9
** statsd-injector-release: v1.3.0
** uaa-fissile-release: 0.0.1-321-g6c32268
* Buildpacks:
** binary-buildpack-release: 1.0.27.1
** dotnet-core-buildpack-release: 1.0.26-14-gf951834
** go-buildpack-release: 1.8.28.1
** java-buildpack-release: 4.16.1-3-g3cf9321
** nodejs-buildpack-release: 1.6.34.1
** php-buildpack-release: 4.3.63.1
** python-buildpack-release: 1.6.23.1
** ruby-buildpack-release: 1.7.26.1
** staticfile-buildpack-release: 1.4.34.1


[id='sec.1_3.issue']
==== Known Issues

* App-AutoScaler will not work on {caasp} without Hairpin enabled.

* Enabling new feature roles, such as CredHub and App-AutoScaler, requires more memory and CPU resources in minimal installations (at least 22 GB in total for single instances that have all roles enabled). If these new feature pods are enabled, for example, on {azure} instances, move to the tier _Standard_D4_v2_ or larger.

* CredHub on {azure} is considered experimental.

* Minibroker with {mariadb} will see timeout issues upon deletion. If an error appears, wait one minute and retry. If the `cf delete-service` command fails but the instance pods are removed from {k8s}, the service instance data can safely be removed with a `cf purge-service-instance` command.

* The {awsa} Service Broker has changed with the recent release of v1.0. The Helm chart from {suse} will be updated in the near future to include these changes.

* The URL of the internal `cf-usb` broker endpoint has changed. To reconnect with {scf}/{product}, brokers for {postgre} and {mysql} that use `cf-usb` will require the following manual fix after the upgrade:
[arabic]
.. Run `kubectl get secret --namespace scf` and copy the name of the secret (for example, `secrets-2.14.5-1`)
.. Run `cf service-brokers` to get the URL for the `cf-usb` host (for example, `https://cf-usb.scf.svc.cluster.local:24054`)
.. Get the current `CF_USB` password by running:
+
[source,bash]
----
kubectl get secret --namespace scf <SECRET_NAME> -o yaml | \
  grep \\scf-usb-password: | cut -d: -f2 | base64 -id
----
+
Replace `<SECRET_NAME>` with the name from the first step.
.. Finally, update the service broker:
+
[source,bash]
----
cf update-service-broker usb broker-admin <PASSWORD> \
  https://cf-usb-cf-usb.scf.svc.cluster.local:24054
----
+
Replace `<PASSWORD>` with the password from step 3. The URL is a modified
version of the URL from step 2: however, as the subdomain name, use
`cf-usb-cf-usb` instead of `cf-usb`.


[id='sec.1_2_1']
=== Release 1.2.1, September 2018


[id='sec.1_2_1.feature']
==== Features and Fixes
* Updated Stratos UI to v2.1
* Updated {scf} to v2.13.3
* Introduction of App-AutoScaler (experimental, off by default)
* Introduction of Minibroker for {redis} (experimental)
* Support for {azure} service brokers
* {cf} deployment bumped to 2.7.0
* `Groot-btrfs` now available
* HA for `nfs-broker`, `cc-clock` and `syslog-scheduler` roles
* Enabled cloud controller security events
* Exposed `broker_client_timeout_seconds` as a router parameter
* Realigned {cf} role composition to be more in line with upstream, which includes these changes:
** `mysql-proxy` has been merged into the `mysql` role
** `diego-locket` has been merged into `diego-api`
** `log-api` roles now combines `loggregator` and `syslog-rlp` 
** `syslog-adapter` renamed as `adapter`
* Removed process list from all roles
* Removed duplicate `routing_api.locket.api_location` property
* `syslog-adapter` added to syslog adapter certificate
* `INTERNAL_CA_KEY` not included in every pod by default
* Better mechanism for waiting on `mysql` included
* Includes these {cf} component versions:
** UAA: v60.2
** cf-deployment: 2.7.0
** ruby-buildpack: 1.7.21.1
** go-buildpack: 1.8.22.1
** kubectl: 1.9.6
** capi-release: 1.61.0
** cflinuxfs2-release: v1.227.0
** cf-mysql-release: v36.15.0
** cf-opensuse42-release: 648e8f1
** cf-sle12-release: c585efc
** cf-smoke-tests-release: 40.0.5
** cf-syslog-drain-release: v7.0
** cf-usb: 7a45076
** consul-release: v195
** diego-release: v2.12.1
** garden-runc-release:  v1.15.1
** loggregator-release: v103.0
** nats-release: v24
** nfs-volume-release: v1.2.0
** postgres-release: v26
** routing-release: 0.179.0
** scf-helper-release: b276460
** cf-acceptance-tests: c83c97b9
** testbrain: 1.0.0-61-ga172cf9
** statsd-injector-release: v1.3.0
** uaa-fissile-release: 0.0.1-299-gdd37ec6
* Buildpacks:
** binary-buildpack-release: 1.0.17
** dotnet-core-buildpack-release: 1.0.26-14-gf951834
** go-buildpack-release: 1.7.19-21-g0897183
** java-buildpack-release: 3.16-18-gfeab2b6
** nodejs-buildpack-release: 1.5.30-13-g584d686
** php-buildpack-release: 3dc85f9
** python-buildpack-release: 1.5.16-14-ga2bbb4c
** ruby-buildpack-release: bd1f612
** staticfile-buildpack-release: 1.4.0-12-gdfc6c09


[id='sec.1_2_1.issue']
==== Known Issues

* Starting with {product} 1.2.1, during helm upgrade, {k8s} will not upgrade pods that are not ready by default. To upgrade all pods, use the complete command: `helm upgrade --force --recreate-pods --version 2.13.3`

* Similar to {caaspa} 3, {azure} now mandates a stricter security policy via PodSecurityPolicy (PSP), which is included as part of the {product} Deployment Guide. Any namespace tied to {product} requires privileged ports to be accessible needs to have to have a PSP set appropriately for access. This would include the default conventions of `scf`, `uaa`, `stratos-ui`, `mysql-sidecar` and `postgres-sidecar` as per our documentation tied to {caaspa} 3: https://documentation.suse.com/suse-cap/1/html/cap-guides/cha-cap-depl-caasp.html#sec-cap-psps

* {azure} users who previously had a {k8s} policy without RBAC, but now have {aks} ({aksa}) with RBAC (which is the new default with {aksa}), will need to modify their `scf-config-values.yaml` files so that `auth: rbac` replaces `auth: none`. If you remain in an {aksa} policy without RBAC, then you can ignore this change.

* If you are using {azure}, ensure that the root partition has enough space for the installation and potential upgrades. To do so, add the parameter `--node-osdisk-size=60` to the command that creates the {aksa} instance: `az aks create`. For the complete command, see the _{product} Deployment Guide_, section _AKS_, subsection _Create Resource Group and AKS Instance_ (https://documentation.suse.com/suse-cap/1/html/cap-guides/cha-cap-depl-aks.html#sec-cap-create-aks-instance).


[id='sec.1_2']
=== Release 1.2, August 2018


[id='sec.1_2.feature']
==== Features and Fixes

* Updated Stratos UI to v2
* Updated {scf} to v2.11.0
* Support for {eks} and {caaspa} v3
* Support for {azure} load balancer enabled
* Updated backup/restore plugin (v1.0.8)
* New active/passive role management for pods whereby the past model of using _Ready_ and _Not Ready_, as states has been retired. Pods will now be labeled as Active or Passive and rely on stateful sets to be managed, allowing for more high availability. Details available here: https://github.com/SUSE/fissile/wiki/Pod-Management-using-Role-Manifest-Tags
* All roles aside from UAA can now be HA
* Certificate expiration now configurable
* Added support for manual rotation of cloud controller database keys
* Exposed the `router.client_cert_validation` property on the router
* Use namespace for helm install name
* Updated the role manifest validation to let the secrets generator use `KUBE_SERVICE_DOMAIN_SUFFIX` without having to configure HA itself
* `SCF_LOG_PORT` now set to default port of 514
* Fixed an issue during upgrade whereby USB sidecars did not receive updated password info, ensuring they will properly communicate with previously registered services
* Patched an issue with the timestamp for `monit_rsyslogd`
* `cf-backup-restore` restores security groups properly now
* `cf-backup-restore` now relies on statically linked Linux binaries
* Includes these {cf} component versions:
** UAA: v59
** cf-deployment: 1.36
** ruby-buildpack: 1.7.18.2
** go-buildpack: 1.8.22.1
** kubectl: 1.8.2
** capi-release: 1.58.0
** cflinuxfs2-release: v1.209.0
** cf-mysql-release: v36.14.0
** cf-opensuse42-release: 054a0ca
** cf-sle12-release: faf946c
** cf-smoke-tests-release: 40.0.5
** cf-syslog-drain-release: v6.5
** cf-usb: 7a45076
** consul-release: v192
** diego-release: v2.8.0-24-gad85f06a
** garden-runc-release:  v1.11.1
** loggregator-release: v102.1
** nats-release: v24
** nfs-volume-release: v1.2.0
** postgres-release: v26
** routing-release: 0.178.0
** scf-helper-release: b276460
** cf-acceptance-tests: 22c36ddc
** testbrain: 1.0.0-61-ga172cf9
** statsd-injector-release: v1.3.0
** uaa-fissile-release: 0.0.1-289-g571836a
* Buildpacks:
** binary-buildpack-release: 1.0.17
** dotnet-core-buildpack-release: 1.0.26-14-gf951834
** go-buildpack-release: 1.7.19-17-g9dbf944
** java-buildpack-release: 3.16-18-gfeab2b6
** nodejs-buildpack-release: 1.5.30-13-g584d686
** php-buildpack-release: 3dc85f9
** python-buildpack-release: 1.5.16-14-ga2bbb4c
** ruby-buildpack-release: ffffb58
** staticfile-buildpack-release: 1.4.0-12-gdfc6c09

[id='sec.1_2.issue']
==== Known Issues

* Upgrading to {product} 1.2 introduces a new active/passive model that will result in a longer-than-usual app instance downtime for upgrades to this new version. As part of this change, you will need to run the `helm upgrade` command with two additional parameters: `helm upgrade --force --recreate-pods --version 2.11.0`. This will be noticeable when seeing {k8s} pods marked as _Unready_. _Unready_ pods will not be upgraded.

* {caaspa} 3 uses an updated version of {k8s} that mandates a stricter security policy via PodSecurityPolicy (PSP) which is included as part of the _{product} Deployment Guide_. This was optional in {caaspa} 2 but it works the same. Any namespace tied to {product} requires privileged ports to be accessible needs to have to have a PSP set appropriately for access. This would include the default conventions of `scf`, `uaa`, `stratos-ui`, `mysql-sidecar` and `postgres-sidecar` as per our documentation.

* UAA should be left as single availability and not high availability (HA)


[id='sec.1_1_1']
=== Release 1.1.1, May 2018

[id='sec.1_1_1.feature']
==== Features and Fixes

* Includes SCF v2.10.1
* Enabled `router.forwarded_client_cert` variable for router
* New syslog roles can have anti-affinity
* {mysql}-proxy healthcheck timeouts are configurable 
* cfdot added to all diego roles
* Removed time stamp check for rsyslog
* Upgrades will handle certificates better by having the required SAN metadata
* Rotatable secrets are now immutable
* Immutable config variables will not be generated
* For high availability (HA) configurations, upgrades no longer require the `api` role to be scaled down
* `cf-backup-restore` handles Docker apps properly now
* `cf-backup-restore` returns a useful error if invalid JSON is parsed 
* PHP buildpack has been bumped to v.4.3.53.1 address MS-ISAC ADVISORY NUMBER 2018-046
* Updated sidecars for {mysql} and {postgre}

* Includes these {cf} component versions:
** uaa: v56.0
** cf-deployment: v.1.21
** loggregator-release: v102.1
** cf-opensuse42-release: 459ef9f
** cf-syslog-drain-release: v6.0
** cf-usb: 79b1a8c
** cf-mysql-release: v36.11.0
** routing-release: 0.174.0
** cf-sle12-release: b96cbc2
** diego-release: v2.1.0
** uaa-fissile-release: 0.0.1-243-ge11bf8d
** cflinuxfs2-release: v1.194.0
** cf-smoke-tests-release: 40.0.1
** nats-release: v23
** scf-helper-release/src/github.com/cloudfoundry/cf-acceptance-tests: 3beb6ed
** capi-release: 1.52.0


[id='sec.1_1_1.issue']
==== Known Issues

* Upgrading now rotates all internal passwords and certificates which may cause some downtime (for example, users will be unable to push applications) as the roles are restarted. This should not impact the availability of hosted applications running multiple instances. 

* If you are using the bundled UAA release, upgrade this first and pass the new certificate to the {scf} upgrade command as outlined in the upgrade instructions below.

* When upgrading, existing deployments of the `cf-usb-sidecar-mysql` or `cf-usb-sidecar-postgres` brokers may subsequently be unable to delete service instances. The following commands fix this problem by updating the internal cf-usb password:

+
[source]
----
CF_NAMESPACE=scf
SECRET=$(kubectl get --namespace $CF_NAMESPACE deploy -o json \
  | jq -r '[.items[].spec.template.spec.containers[].env[] \
  | select(.name == "INTERNAL_CA_CERT").valueFrom.secretKeyRef.name] \
  | unique[]')
USB_PASSWORD=$(kubectl get -n scf secret $SECRET -o jsonpath='{@.data.cf-usb-password}' \
  | base64 -d)
USB_ENDPOINT=$(cf curl /v2/service_brokers \
  | jq -r '.resources[] | select(.entity.name=="usb").entity.broker_url')
cf update-service-broker usb broker-admin "$USB_PASSWORD" "$USB_ENDPOINT"
----

* If after upgrading:
** the `diego-api` role is not fully functional (i.e. appearing as `(0/1)`)
** the `bbs` job in the pod is not starting (as per `monit summary`)
** the bbs stdout log `/var/vcap/sys/log/bbs/bbs.stdout.log` contains _Error 1062: Duplicate entry 'version' for key 'PRIMARY'_
+
Do the following to unblock the upgrade:
** `kubectl exec` into (one of) the mysql pod(s)
+
----
kubectl exec -it mysql-0 --namespace cf -- env TERM=xterm /bin/bash
----
** Use `mysql` to connect to the diego database
+
----
mysql --defaults-file=/var/vcap/jobs/mysql/config/mylogin.cnf diego
----
** Remove the offending entry
+
----
DELETE FROM configurations WHERE id='version';
----

* Do not set the `mysql-proxy`, `routing-api`, `tcp-router`, `blobstore` or
`diego_access` roles to more than one instance each. Doing so can cause problems
with subsequent upgrades which could lead to loss of data. Scalability of these
roles will be enabled in an upcoming maintenance release.
* The `diego-api`, `diego-brain` and `routing-api` roles are configured as
active/passive, and passive pods can appear as _Not Ready_. This is expected
behavior.
* {azure} operators may not be able to connect to {azure} Database for
{mysql}/{postgre} databases with the current brokers.


[id='sec.1_1']
=== Release 1.1, April 2018


[id='sec.1_1.new']
==== What Is New?

* Now supported on Microsoft Azure Container Services (AKS)
* Cloud Foundry component and buildpack updates (see <<sec.1_1.feature>>)
* {postgre} and {mysql} service broker sidecars, configured and deployed via Helm
* cf backup+ CLI plugin for saving, restoring, or migrating CF data and
applications

For more information about deploying {product}, see the _Deployment Guide_ at
{deployment-url}.


[id='sec.1_1.feature']
==== Features and Fixes

* Includes SCF v2.8.0
* Ability to specify multiple external IP addresses (see <<sec.1_1.issue>>
  below on impact to upgrades)
* {mysql} now a clustered role
* {mysql}-proxy enabled for UAA
* UAA has more logging enabled, so `SCF_LOG_HOST`, `SCF_LOG_PORT` and
  `SCF_LOG_PROTOCOL` have been exposed
* TCP routing ports are configurable and can be templatized
* CPU limits can be set for pods.
* Memory limits for pods now properly enforced.
* {k8s} annotations enabled so operators can specify what nodes
  particular roles can be run on
* Fixed cloud controller clock so that it will wait until API is ready
* Overhauled secret rotation for upgrades

* Includes these CF component versions:
** diego-release 1.35
** cf-mysql-release 36.10.0
** cflinuxfs2-release 1.187.0
** routing-release 0.172.0
** garden-runc-release 1.11.1
** nats-release 22
** capi-release 1.49.0

* Includes these {cf} buildpack versions:
** go-buildpack-release 1.7.19-16-g37cc6b4
** binary-buildpack-release 1.0.17
** nodejs-buildpack-release 1.5.30-13-g584d686
** ruby-buildpack-release 9adff61
** php-buildpack-release ea8acd0
** python-buildpack-release 1.5.16-14-ga2bbb4c
** staticfile-buildpack-release 1.4.0-12-gdfc6c09
** dotnet-core-buildpack-release 1.0.26-14-gf951834
** java-buildpack-release 3.16-18-gfeab2b6


[id='sec.1_1.configuration']
==== Configuration Changes

Changes to the format of `values.yaml` for SCF and UAA require
special handling when upgrading from {product} 1.0 to 1.1 if you are reusing
configuration files (for example, `scf-config-values.yaml`):

* All secrets formerly set under `env:` are now set under `secrets:`.
Any `_PASSWORD`, `_SECRET`, `_CERT`, or `_KEY` value explicitly set in
`values.yaml` for {product} 1.0 should be moved into the `secrets:` section
before running `helm upgrade` with the revised `values.yaml`. Find a sample
configuration in <<app.secret-sample>>.

* **These secrets must be resupplied on each upgrade** (for example, the
`CLUSTER_ADMIN_PASSWORD`, `UAA_ADMIN_CLIENT_SECRET`) as they will not be carried
forward automatically. We recommend always using a values file.

* To rotate secrets, increment the `kube.secrets_generation_counter`
(immutable generated secrets will not be reset).

* The `kube.external_ip` variable has been changed to `kube.external_ips`,
allowing for services to be exposed on multiple {k8s} worker nodes (for
example, behind a TCP load balancer). Before upgrading, change the setting or
add a new setting specified as an array. For example:
+
----
kube.external_ip=10.1.1.1
kube.external_ips=["10.1.1.1"]
----

* Both variables can exist at the same time and be set to the same value for
those in mixed version environments. To specify multiple addresses, use:
+
[source]
----
kube.external_ips=["1.1.1.1", "2.2.2.2"]
----

* Upgrading from {product} 1.0.1 to 1.1
+
An example `scf-config-values.yaml` for {product} 1.1 would look like this:
+
[source,yaml]
----
env:
    # Domain for SCF. DNS for *.DOMAIN must point to a kube node's (not master)
    # external ip address.
    DOMAIN: cf-dev.io

kube:
    # The IP address assigned to the kube node pointed to by the domain.
    #### the external_ip setting changed to accept a list of IPs, and was
    #### renamed to external_ips
    external_ips: ["192.168.77.77"]
    storage_class:
        # Make sure to change the value in here to whatever storage class you use
        persistent: "persistent"
        shared: "shared"

    # The registry the images will be fetched from. The values below should work for
    # a default installation from the suse registry.
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap"

    auth: rbac

secrets:
    # Password for user 'admin' in the cluster
    CLUSTER_ADMIN_PASSWORD: changeme

    # Password for SCF to authenticate with UAA
    UAA_ADMIN_CLIENT_SECRET: uaa-admin-client-secret
----
+
To upgrade from {product} 1.0.1 to 1.1, run the following commands:
+
[source,bash]
----
$ helm repo update
$ helm upgrade --recreate-pods <uaa-helm-release-name> suse/uaa --values scf-config-values.yaml --version 2.8.0
$ SECRET=$(kubectl get pods --namespace uaa -o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')
$ CA_CERT="$(kubectl get secret $SECRET --namespace uaa -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
$ helm upgrade --recreate-pods <scf-helm-release-name> suse/cf --values scf-config-values.yaml --set "secrets.UAA_CA_CERT=${CA_CERT} --version 2.8.0"
$ helm upgrade --recreate-pods <console-helm-release-name> suse/console --values scf-config-values.yaml --version 1.1.0
----


[id='sec.1_1.issue']
==== Known Issues

IMPORTANT: You will need Stratos UI 1.1 when running {product} 1.1 and you
share the `scf-values.yaml` configuration file between them. Prior versions
of the Stratos UI will not work.

IMPORTANT: If you have used a configuration file from a version prior to
1.1, you will need to update it. See details below.

* The variable `kube.external_ip` has now been renamed to
`kube.external_ips`, meaning upgrades from older versions will fail unless
the latter variable exists in the `scf-values.yaml` file used to deploy
{product}. Both variables can exist at the same time and be set to the same
value for those in mixed version environments:
+
[source]
----
kube.external_ip=1.1.1.1
kube.external_ips=[1.1.1.1]
----

** Going forward, `kube.external_ips` is an array, hence it can be used as
reproduced below:
+
[source]
----
kube.external_ips=[“1.1.1.1”, “2.2.2.2”]
----

** Also as a result of this change, the `helm` command line client must be version 2.6.0 or higher.

** All the secrets have been renamed from `env.FOO` to `secrets.FOO`, so all
the appropriate entries in `scf-values.yaml` need to be modified to align with
that change.

** You need to keep specifying *all* your secrets on each upgrade (for example,
the `CLUSTER_ADMIN_PASSWORD`) as it will not be carried forward automatically.

** To rotate secrets, increment the `kube.secret_generation_counter`. Note
  that immutable generated secrets will not be reset.

* In HA environments, upgrades can run into an issue whereby the API pods do
  not all come up post-migration. The work around this issue, before the
  upgrade, scale down the API role to 1. After completing the upgrade, scale
  the API role up again to 2 or more.

** Some roles (like diego-api, diego-brain and routing-api) are configured as
active/passive, so passive pods can appear as `Not Ready`.

** Other roles (tcp-router and blobstore) cannot be scaled.

* Cloud Application Platform v1.1 requires that Stratos UI use version 1.1.
Older versions of the UI will not work due to the change in variable names.

* Azure operators may not be able to connect to SQL databases with the sidecar.

* Restores performed by the Backup CLI may leave docker apps in a stopped state.
The workaround is to restart the affected applications.

* A proper JSON file generated by the Backup CLI needs to be provided in order
to do a restore, otherwise an ugly error appears.

* Do not set the `mysql-proxy`, `routing-api`, `tcp-router`, `blobstore` or
`diego_access` roles to more than one instance each. Doing so can cause problems
with subsequent upgrades which could lead to loss of data. Scalability of these
roles will be enabled in an upcoming maintenance release.
* To upgrade high availability (HA) configurations, scale down the `api`
role count to 1. Then upon completing the upgrade, scale `api` up again to
2 or more.
** The `diego-api`, `diego-brain` and `routing-api` roles are configured as
active/passive, and passive pods can appear as _Not Ready_. This is expected
behavior.
* Azure operators may not be able to connect to Azure Database for
{mysql}/{postgre} databases with the current brokers.
* `cf backup-restore` may leave Docker apps in a stopped state. These can be
started manually.
* `cf backup-restore` produces an unhelpful error if the file is not valid JSON.


[id='sec.1_0_1']
=== Release 1.0.1, February 2018

[id='sec.1_0_1.feature']
==== Features and Fixes

* Using the `helm upgrade` command in {product} 1.0 to 1.0.1 (scf 2.6.11 to
  2.7.0) requires the use of `--force` to drop an unnecessary persistent
  volume. Note that `helm upgrade` only works for multi-node clusters when
  running with a proper HA storage class. For example, `hostpath` will not
  work, as required stateful data can be lost.
* Bump to {cf} Deployment (1.9.0), using {cf} Deployment not {cf} Release
  from now on
* Bump UAA to v53.3
* Add ability to rename immutable secrets
* Update CATS to be closer to what upstream is using
* Make RBAC the default in the values.yaml (no need to specify anymore)
* Increase test brain timeouts to stop randomly failing tests
* Remove unused SANs from the generated TLS certificates
* Remove the dependency on jq from stemcells
* Fix duplicate buildpack ids when starting {cf}
* Fix an issue in the vagrant box where compilation would fail due to old
  versions of docker.
* Fix an issue where diego cell could not be mounted on NFS-backed {k8s}
  storage class
* Fix an issue where diego cell could not mount NFS in persi
* Fix several problems reported with the syslog-forwarding implementation


[id='sec.1_0_1.issue']
==== Known Issues

* Do not set the `mysql` or `diego_access` roles to more than one instance each
in HA configurations. Doing so can cause problems with subsequent upgrades
which could lead to loss of data. Scalability of these roles will be enabled
in an upcoming maintenance release.

* A `helm upgrade` command from 1.0 to 1.0.1 (scf 2.6.11 to 2.7.0) requires the
use of `--force` to drop an unnecessary persistent volume. Note that
`helm upgrade` only works for multi-node clusters when running with a proper
HA storage class (for example, `hostpath` will not work as required stateful
data can be lost).


[id='sec.1_0']
=== Release 1.0, January 2018

* Initial product release
